<span id="slides-title" hidden>Классификаторы</span>

Это тоже относится к машинному обучению =)

= = = = = =

Наивный байесовский классификатор
=================================

- - - - - -

## Совсем простой пример

1. Приходит электронная почта
2. Письмо отправляется в [СПАМ](https://en.wikipedia.org/wiki/Email_spam) или нет по решению пользователя
    * Если письмо отправляется в СПАМ, всем словам, из которых состоит его текст, **убавляется репутация**
    * Если письмо хорошее, всем словам **репутация прибавляется**
3. По прошествии некоторого времени система предлагает пользователю решения, а пользователь соглашается, или нет
4. По прошествии ещё какого-то времени пользователя больше не спрашивают


-{pause}-

Система может остановить обучение, или доучиваться постепенно в расчёте на то, что СПАМ будет медленно эволюционировать

- - - - - -

## Другая частая ситуация

Требуется не бинарная, а более тонкая классификация


- - - - - -

Постановка задачи
-------

Пусть есть множество объектов $O$. Объекты $o \in O$.

Есть набор категорий $C$. И категории $c \in C$.

Требуется классифицировать объект:

$$ c(o) = \arg\max_{c \in C} \mathrm{P}(c | o)$$

-{pause}-

Или просто вычислить вероятности $\mathrm{P}(c | o), c \in C$

- - - - - -

## Ещё немного теории

Теорема [Байеса](https://en.wikipedia.org/wiki/Thomas_Bayes):


$$ \mathrm{P}(c|o) = \frac{\mathrm{P}(o|c)\mathrm{P}(c)}{\mathrm{P}(o)} $$


- - - - - -

## Признаки (features) объекта

Например, наличие определённых слов в тексте. А чаще не слов, а [N-грамм](https://en.wikipedia.org/wiki/N-gram).

Для объекта $o \in O$ — $f_1,\ldots,f_N$.

-{pause}-

$$\mathrm{P}(o) = \mathrm{P}(f\_1,\ldots,f\_N) = \prod\_{i=1}^N\mathrm{P}(f\_i)$$

-{pause}-

$$\ldots = 1,$$

когда мы работаем с объектом $o$, т.к. у него все эти признаки есть.

- - - - - -

## И формула упрощается

$$\mathrm{P}(c|o) = \mathrm{P}(c) \prod\_{i=1}^N\mathrm{P}(f\_i | c) $$

-{pause}-

А если хочется вынести окончательный вердикт, то

$$ c(o) = \arg\max_{c \in C} \mathrm{P}(c|o).$$

- - - - - -

## Было использовано довольно сильное допущение

О том, что

$$f_1, \ldots, f_N$$

независимы. А на самом деле это не так.

-{pause}-

Пример: «темно, как у ... в ...» — вроде бы может быть, что угодно, а общепринятых вариантов не так много

- - - - - -

## Пример ([Отсюда](https://habr.com/ru/post/120194/)): обучение


```
from collections import defaultdict

def train(training_data):
    categorie_powers = defaultdict(lambda: 0)
    frequencies = defaultdict(lambda: 0)

    for feature_list, category in training_data:
        categorie_powers[category] += 1
        for feature in feature_list:
            frequencies[category, feature] += 1


    # p(o|c)
    for category, feature in frequencies.keys():
        frequencies[category, feature] /= categorie_powers[category]

    # p(c)
    for c in categorie_powers.keys():
        categorie_powers[c] /= len(training_data)

    return categorie_powers, frequencies

```

- - - - - -

## Пример: классификация (упрощённо, [линейная классификация](https://en.wikipedia.org/wiki/Linear_classifier))


```
def classify_lineary(knowledge, feature_counts):
    c_probs, o_probs = knowledge

    r = defaultdict(lambda: 0)
    for c in c_probs.keys():
        for f in feature_counts.keys():
            r[c] += c_probs[c] * o_probs[c, f] * feature_counts[f]

    return r
```

-{pause}-

Тут есть лукавство (точнее откровенное враньё). Код написан так, как будто

$$\mathrm{P}(c|o) = \mathrm{P}(c) \sum\_{i=1}^N\mathrm{P}(f\_i | c),$$

А там же произведение, а не сумма.

- - - - - -

## Пример: использование (лукавство всё ещё с нами!)


```
k = train([
    (["мясо", "рыба", "потроха"], "мясоед"),
    (["молоко", "рыба", "яйца", "репа"], "умеренный вегетерианец"),
    (["вода", "укроп", "репа"], "веган")
])

print(dict(classify_lineary(k, {"мясо": 4, "рыба": 3})))

print(dict(classify_lineary(k, {"потроха": 1, "репа": 5})))

```

Выдаёт

```
{'мясоед': 2.333333333333333, 'умеренный вегетерианец': 1.0, 'веган': 0.0}
{'мясоед': 0.3333333333333333, 'умеренный вегетерианец': 1.6666666666666665, 'веган': 1.6666666666666665}
```

- - - - - -

## Избавимся от лукавства

$$\mathrm{P}(c|o) = \mathrm{P}(c) \prod\_{i=1}^N\mathrm{P}(f\_i | c) = \mathrm{P}(c) \mathrm{exp}\left(\sum\_{i=1}^N\ln\mathrm{P}(f\_i | c)\right) $$


```
import math

def robust_log(v):
    if v > 1e-10:
        return math.log(v)
    else:
        return -1e20

def classify(knowledge, feature_counts):
    c_probs, o_probs = knowledge

    r = defaultdict(lambda: 0)
    for c in c_probs.keys():
        for f in feature_counts.keys():
            r[c] += robust_log(o_probs[c, f]) * feature_counts[f]

    for c in c_probs.keys():
        r[c] = c_probs[c] * math.exp(r[c])

    return r
```

- - - - - -

# Попробуем

```
print(dict(classify(k, {"мясо": 4, "рыба": 3})))
print(dict(classify(k, {"потроха": 1, "репа": 5})))

```

```
{'мясоед': 0.3333333333333333, 'умеренный вегетерианец': 0.0, 'веган': 0.0}
{'мясоед': 0.0, 'умеренный вегетерианец': 0.0, 'веган': 0.0}
```

- - - - - -

## Ссылки

Доходчивые и простые

1. [Тоби Сегаран. Программируем коллективный разум: \[пер. с англ.\] // М. — Символ-Плюс, 2008, 368 с.](https://books.google.ru/books?isbn=5932861193)
2. http://bazhenov.me/blog/2012/06/11/naive-bayes.html
3. https://habr.com/ru/post/120194/

- - - - - -

# Cовсем оставили за кадром

* 90% из перенесших инфаркт граждан СССР ели капусту
* Все герои предыдущих слайдов пьют воду

-{pause}-

## [Модеть TF * IDF из информационного поиска](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

* TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа
* IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции

Т.е. если слово встречается (почти) везде, надо либо снижать его вес, как признака, либо исключать его из признаков

= = = = = =

Что может и чего не может НБК?
==============================

- - - - - -

```
    for c in c_probs.keys():
        for f in feature_counts.keys():
            r[c] += c_probs[c] * o_probs[c, f] * feature_counts[f]

```

Так это же...

-{pause}-

... првильно, ***линейная классификация***, т.е.

коль скоро $f$ — `feature_counts` — вектор, $c$ — `c_probs` — вектор, а $O$ — `o_probs` — матрица, то:

$$ r = c^{T} \times O \times f$$

- - - - - -

### Что он может

<div style="text-align: center;">

![AD](images/12.linear-can.svg) <!--.element: style="height: 30%;" -->

</div>

-{pause}-

Там же логарифм был! Значит по кривой, но всё равно одинаково и монотонно

<div style="text-align: center;">

![AD](images/12.linear-can-log.svg) <!--.element: style="height: 30%;" -->

</div>


- - - - - -

## А вот так он уже совсем не может

<div style="text-align: center;">

![AD](images/12.linear-cant.svg) <!--.element: style="height: 40%;" -->

</div>

- - - - - -

## Известная картинка

<div style="text-align: center;">

![AD](images/12.wikipedia.Kernel_Machine.svg) <!--.element: style="height: 40%;" -->

</div>

Из Википедии

-{pause}-

Линейный классификатор может работать, когда данные уже такие, как справа. Потому, что он **линейный**.

= = = = = =

# Ступенчатая аппроксимация функции

- - - - - -

## Синус

<div style="text-align: center;">

![AD](images/12.linear-sin-1.svg) <!--.element: style="height: 50%;" -->

</div>

- - - - - -

## Аппроксимация синуса

<div style="text-align: center;">

![AD](images/12.linear-sin-2.svg) <!--.element: style="height: 50%;" -->

</div>

- - - - - -

## Чем можно аппроксимировать?

* Прямоугольниками
* «Колокольчиками»

-{pause}-

* «Половинками» от того и другого

- - - - - -

## Почти половинка от колокольчика

Сигмоида

<div style="text-align: center;">

![AD](images/12.wikipedia.Logistic-curve.png) <!--.element: style="height: 600px;" -->

</div>

$$\sigma(x) = \frac{1}{1 + e^{-x}},\; \frac{d\sigma(x)}{d x} = \sigma(x) (1 - \sigma(x))$$

= = = = = =

# Градиентный спуск

- - - - - -

## В целом

Умеем вычислять $f(x)$, но знаем мало её свойств. Надо найти $\arg\min_x f(x)$.

-{pause}-

Берём случайный $x$, а дальше

$$x \gets - \lambda\nabla f(x)$$

- - - - - -

## Теперь давайте чуть иначе

### Не по $x$, а по параметрам


$$f(x) \approx f^{\sim}(x) = c \sum\_{i=1}^{N} \sigma(a_i(x + b_i))$$

Надо подобрать $c$ и $a_i$ с $b_i$ в количестве $2N$ штук.

### Будем не минимизировать функцию, а пытаться её приблизить к нужной

$$x \gets - \lambda\nabla_b (f^{\sim}(x) - f(x))$$

- - - - - -

## Чуть упростим задачу и модель

Для аппроксимации монотонно неубывающей на отрезке $[0, \pi/2]$ хватит

$$f^{\sim} (x) = \frac{1}{N} \sum\_{i=1}^{N} \sigma\left(a (x - b_i)\right)$$

Обращаем внимание:

1. Именно минус ($x - b_i$), т.е. $b_i$ смещают сигмоиды вправо
2. $a$ — константа, чтобы сделать сигмоиды «круче», например $\frac{100 N}{\pi}$

- - - - - -

## Градиентный спуск по $b_i$

Итерируемся, и для различных $x$ берём:

$$b_i \gets b_i + \lambda \frac{\partial (f(x) - f^{\sim}(x))}{\partial b_i}$$

- - - - - - -

## Попробуем уже наконец аппроксимировать синус!

Будем аппроксимировать на $[0, \pi/2]$

Попытаемся найти $b_i$... т.е. кусочки сигмоид будем смещать только по горизонтали

Исходник [тут](images/src/12.approx-sin.py).

- - - - - -

### Начало

<div style="text-align: center;">

![AD](images/12.approx_sin_initial.svg) <!--.element: style="height: 800px;" -->

</div>

- - - - - -

### Процесс...

<div style="text-align: center;">

![AD](images/12.approx_sin.svg) <!--.element: style="height: 800px;" -->

</div>

- - - - - -

### Победа!

<div style="text-align: center;">

![AD](images/12.approx_sin_final.svg) <!--.element: style="height: 800px;" -->

</div>


- - - - - - -

## Теперь давайте аппроксимировать композицию

$$ z = g(y), y = f(x), \text{т.е.}\; z = (g \circ f)(x)$$

Причём мы тоже берём разные $x$, но на каждой итерации $x$ фиксирован, а аппроксимируем мы наборы (вектора) параметров $\alpha$ для $f$ и $\beta$ для $g$,
которыми задаются сами функции.

-{pause}-

Т.е. надо для фиксированных $х$:

* Вычислить по нему $y$ и $z$
* При помощи $\nabla g$ вычислить, насколько должен измениться $\beta$, т.е. получить $\Delta y$

Но ведь у нас есть ещё $\frac{\partial g}{\partial \alpha}$, [их тоже надо как-то учесть](https://neurohive.io/ru/osnovy-data-science/obratnoe-rasprostranenie/).

-{pause}-

$$\frac{\partial g}{\partial \alpha} = \frac{\partial g}{\partial \beta} \cdot \frac{\partial \beta}{\partial \alpha}$$

* Следующие значения:
    * $\beta \gets \beta + \lambda \nabla\_\beta g(f(x))$
    * $\alpha \gets \alpha + \lambda \nabla\_\beta g(f(x)) \nabla\_\alpha f(x)$

- - - - - -

## То, что получилось...

... позволяет по данным об ошибке улучшить параметры, которыми задаются функции.

- - - - - -

## А если x...

А если $x$ — не число, а ветктор, что делать?

1. Можно делать то же самое (применять сигмоиду и смотреть на градиент), но покомпонентно.
2. Связывать аргументв при композиции тоже через какой-то «коэффициент»

-{pause}-

А что такое «коэффициент» для вектора в самом общем виде?

-{pause}-

Правильно, матрица! Т.е. линейный оператор.

- - - - - -

## Итак, система в наиболее общем виде

Результат:

$$z = \sigma(y_l)$$

-{pause}-

$$y_l = w^{xy} y$$

-{pause}-

$$y = \sigma(x),$$

Где $x$ — вектор с исходными данными

= = = = = =

# А теперь примеры с нейронными сетями

- - - - - -

## Примеры с исключающим «или», импликацией и простыми фигурами

[Пример с исключающим «или» и импликацией](https://github.com/dluciv/lections-everywhere/tree/master/jupyter-notebooks/10.Neural)


-{pause}-


Почему исключающее «или»? Потому, что оно от двух переменных задаёт «седловидную» поверхность, т.е. оно явно требует нелинейного решателя.

-{pause}-

Примеры с [Keras](https://keras.io/) там же
