{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простейшая нейронная сеть\n",
    "\n",
    "Пример инспирирован: https://github.com/stmorgan/pythonNNexample\n",
    "\n",
    "Видео: https://youtu.be/h3l4qz76JhQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция активации и её производная\n",
    "\n",
    "Производная для градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array as na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции numpy и её операции покомпонентно работают с векторами\n",
    "\n",
    "def σ(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dσdx(x):\n",
    "    return σ(x)*(1-σ(x))\n",
    "\n",
    "λ = 0.15  # тормозилка\n",
    "iterations = 40000  # зубрилка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучающие данные. Во входном слое также есть т.н. *нейрон смещения*, который всегда 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Входные данные — набор векторов-столбцов\n",
    "X = [\n",
    "    na([[0],[0],[1]]),\n",
    "    na([[0],[1],[1]]),\n",
    "    na([[1],[0],[1]]),\n",
    "    na([[1],[1],[1]])\n",
    "]\n",
    "\n",
    "# Выходные данные — XOR и `=>` — тоже набор векторов-столбцов\n",
    "Z = [\n",
    "    na([[0], [1]]),\n",
    "    na([[1], [1]]),\n",
    "    na([[1], [0]]),\n",
    "    na([[0], [1]])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем линейные операторы, имитирующие синапсы, случайными значениями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "# Инициализируем матрицы весов (соответствуют синапсам нейронной сети)\n",
    "W_0 = 2 * np.random.random((4,3)) - 1  # Матрица весов 4x3 — 4 нейрона в скрытом слое из 3 входных нейронов\n",
    "W_1 = 2 * np.random.random((2,4)) - 1  # Матрица весов 2x4 — 2 выходных нейрона из 4 скрытых"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сети:\n",
    "\n",
    "<img src=\"img/neural1.png\" style=\"width: 50%;\" />\n",
    "\n",
    "Обучение с обратным распространением ошибки:\n",
    "\n",
    "<img src=\"img/backpro1.png\" style=\"width: 50%;\" />\n",
    "\n",
    "\n",
    "Запустим в цикле обучение, повторяя одни и те же обучающие примеры "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.23139668302794075\n",
      "Error: 0.0005610036910077208\n",
      "Error: 8.87985676657125e-05\n",
      "Error: 4.701233539415214e-05\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    for x, z in zip(X, Z):\n",
    "    \n",
    "        # Применим нейронную сеть\n",
    "        l_0 = x\n",
    "        l_1 = σ(W_0 @ l_0)\n",
    "        l_2 = σ(W_1 @ l_1)\n",
    "        \n",
    "        # Обратное распространение ошибки\n",
    "        \n",
    "        # ---- Выходной слой ----\n",
    "        \n",
    "        ε_l_2 = z - l_2                 # ε_l_2 — ошибка (со знаком минус) на выходе —\n",
    "                                        # сколько надо минус сколько получилось.\n",
    "        \n",
    "        Δ_i_2 = λ * ε_l_2 * dσdx(l_2)   # Как должны были измениться входы нейронов выходного слоя\n",
    "                                        # градиентный спуск в чистом виде. Несмотря на то, что у нас dσ/dx,\n",
    "                                        # фактически мы используем её, как производную по линейным операторам\n",
    "                                        # нейронной сети, а x у нас на каждой итерации фиксирован.\n",
    "\n",
    "        # ---- Скрытый слой ----\n",
    "            \n",
    "        ε_l_1 = W_1.T @ Δ_i_2           # Неочевидное место (1) — как мы считаем ошибку (со знаком минус)\n",
    "                                        # в скрытом слое.\n",
    "\n",
    "        Δ_i_1 = λ * ε_l_1 * dσdx(l_1)   # Как должны были измениться входы нейронов скрытого слоя\n",
    "                                        # опять градиентный спуск в чистом виде.\n",
    "    \n",
    "        # ---- Коррекция весов ----\n",
    "        \n",
    "        W_1 += Δ_i_2 @ l_1.T            # Неочевидное место (2) — \n",
    "        W_0 += Δ_i_1 @ l_0.T            # Как мы корректируем операторы\n",
    "    \n",
    "    if j % 10000 == 0:   # Раз в 1000 итераций печатаем среднее значение квадрата отклонения \n",
    "        print(f\"Error: {np.square(ε_l_2).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неочевидные места (линейные индексы пишем наверху)\n",
    "\n",
    "Связаны с тем, что мы пытаемся пройти обратно не «через обратные функции и обратные матрицы», а «через производные функций и линейных операторов». Потому что прямо идёт сигнал, а обратно идёт ошибка, и мы хотим смоделировать ситуацию, при которой изменения в параметрах будут вызывать нужные изменения в ошибке.\n",
    "\n",
    "1. $\\varepsilon l_1 = W_1^T \\times \\Delta i_2$. Мы хотим выяснить, насколько ошибся слой $l_1$ (ошибся в том, что вызвало ошибку в следующем слое =)).\n",
    "    * $i_2^i = \\sum_j W_1^{i,j} l_1^j$.\n",
    "    * Мы видим, что приращение $\\varepsilon l_1^{j}$, приводит к изменениям $\\Delta i_2^i = W_1^{i,j} l_1^{j}$. И поэтому мы в $l_1^j$ «отправляем» назад по тем же связям получившуюся в $i_2$ ошибку в виде $\\sum_i W_1^{i,j} \\Delta i_2^i$.\n",
    "2. `W_1 += Δ_i_2 @ l_1.T`. Примерно та же идея.\n",
    "    * $W_1$ — насколько сильно выходы $l_1$ действуют на входы $i_2$, а именно $i_2^i = \\sum_j w_1^{i,j} l_1^j$.\n",
    "    * Соответственно, если кто-то $j$-й подействовал ($l_1^j$) неправильно на $i$-го ($\\Delta_{i_2}^i$), мы это учтём пропорциолнально тому, какую ошибку это вызвало, и насколько он сам был возбуждён.\n",
    "    * И скорректируем их связи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте протестируем сеть..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00958842 0.99867086]\n",
      "[0.98627491 0.99999998]\n",
      "[0.9917872  0.00367186]\n",
      "[0.00888672 0.99826763]\n"
     ]
    }
   ],
   "source": [
    "def predict(arg):\n",
    "    arg += [1]\n",
    "    l_1 = σ(W_0 @ arg)\n",
    "    l_2 = σ(W_1 @ l_1)\n",
    "    return l_2\n",
    "\n",
    "print(predict([0,0]))\n",
    "print(predict([0,1]))\n",
    "print(predict([1,0]))\n",
    "print(predict([1,1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
